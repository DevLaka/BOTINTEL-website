<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <!-- // CSS FILES // -->

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="bootstrap-4.1.1-dist/css/bootstrap.min.css" />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/icon?family=Material+Icons"
    />
    <link
      rel="stylesheet"
      href="https://use.fontawesome.com/releases/v5.3.1/css/all.css"
      integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU"
      crossorigin="anonymous"
    />
    <!-- OWl Carousel CSS Files -->
    <link rel="stylesheet" href="js/plugins/owl-carousel/owl.carousel.css" />
    <link rel="stylesheet" href="js/plugins/owl-carousel/owl.theme.css" />
    <link rel="stylesheet" href="js/plugins/owl-carousel/owl.transitions.css" />
    <!-- ANIMATE CSS -->
    <link rel="stylesheet" href="css/animate.css" />
    <!-- Video Pop Up Plugin -->
    <link
      rel="stylesheet"
      href="js/plugins/YouTube_PopUp-master/YouTubePopUp.css"
    />
    <!-- PRELOADER -->
    <link rel="stylesheet" href="css/preloader.css" />
    <!-- Main CSS -->
    <link rel="stylesheet" href="css/style.css" />
    <!-- <link href="css2/style.css" rel="stylesheet"> -->
    <link href="css2/dharestyle.css" rel="stylesheet" />
    <title>Bot Intel</title>
  </head>

  <body data-spy="scroll" data-target=".navbar">
    <!-- ==============================================
    HEADER
    =============================================== -->

    <header id="home">
      <!-- /// Navbar /// -->

      <nav class="navbar navbar-expand-lg fixed-top">
        <div class="container">
          <!-- // Brand // -->

          <a class="navbar-brand" href="index.html">Bot Intel</a>
          <button
            class="navbar-toggler"
            type="button"
            data-toggle="collapse"
            data-target="#navbarNav"
            aria-controls="navbarNav"
            aria-expanded="false"
            aria-label="Toggle navigation"
          >
            <i class="material-icons">menu</i>
          </button>
          <div class="collapse navbar-collapse" id="navbarNav">
            <!-- / NavLinks / -->

            <ul class="nav navbar-nav ml-auto">
              <li class="nav-item">
                <a class="nav-link page-scroll" href="index.html">Home</a>
              </li>
              <li class="nav-item">
                <a class="nav-link page-scroll" href="source.html">Scope</a>
              </li>
              <li class="nav-item">
                <a class="nav-link page-scroll" href="milestones.html"
                  >Milestones</a
                >
              </li>
              <li class="nav-item">
                <a class="nav-link page-scroll" href="doc.html">Documents</a>
              </li>
              <li class="nav-item">
                <a class="nav-link page-scroll" href="Presentation.html"
                  >Presentations</a
                >
              </li>
              <li class="nav-item">
                <a class="nav-link page-scroll" href="about.html">About Us</a>
              </li>
              <li class="nav-item">
                <a class="nav-link page-scroll" href="contact.html"
                  >Contact Us</a
                >
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- /// BANNER /// -->
      <div class="banner">
        <div class="container">
          <div class="row">
            <div class="col-md-8 offset-md-2">
              <!-- // Caption // -->
              <div class="caption" style="padding-top: 150px">
                <h2 style="color: aliceblue; padding-bottom: 25px">
                  Research Domain
                </h2>
                <p style="color: aliceblue; padding-bottom: 40px">
                  Home > Scope
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </header>

    <section id="services" style="padding-bottom: 50px">
      <div class="container">
        <div class="row">
          <div
            class="col-md-12 wow fadeInLeft"
            data-wow-duration=".5s"
            data-wow-delay=".2s"
          >
            <h3>Literature Survey</h3>
            <p>
              An intelligent service robot is a machine that is able to gather
              information from its environment and use its knowledge to operate
              safely in a meaningful and purposive manner. Recent developments
              in intelligent service robotics have opened up new areas of
              robotic applications, including service robots designed
              specifically for domestic correspondence. According to the latest
              statistical research conducted by Kenneth Research [1], world
              domestic service robots, including both personal and household
              service robotics, have witnessed a steady growth in the recent
              years owing to the wider awareness of robotics applications,
              increasing cost for human service labor, and continuous
              technological advancement.
              <br /><br />

              Most importantly, the users of these intelligent service robots
              are ordinary human users who do not have expert knowledge of
              robotics. At present, every gadget comes with a manual and user
              have to act accordingly. This is a tedious task for the user. In
              relation to an intelligent service robot, referring to a manual
              just to communicate with the robot will reduce the
              user-friendliness tremendously. Thus, to provide complex services
              with a better satisfactory level the robot should compose
              user-friendly and human-like communication capabilities [2]-[5].
              It requires integrating of social cognitive features of humans to
              robots [2]. However, it is a very challenging task.
              <br /><br />

              When considering the current systems, it is comprehensible that
              there is a lot more to improve. Humans use uncertain terms related
              to movement speeds, object sizes, distance-related terms,
              time-related terms, etc. unintentionally in natural language
              instructions. Contrarily, the existing systems are mostly focusing
              on dealing with uncertain terms related to distances such as
              “near” and “far” [6]-[9], direction of objects[10], the object
              sizes such as “big” and “small” [11], [12]. Uncertain terms
              associated with time such as “later”, movement speeds “slowly” are
              not covered in existing systems.
              <br /><br />
              Moreover, in order to relax the issue of interpreting unknown
              words in a natural language instruction given to a domestic
              service robot, knowledge enabled programming approaches have been
              suggested through many researches. Here, the most common measure
              is to represent the relevant terms using ontology languages that
              conceptualize the respective domain [13],[14],[15]. Moreover, in
              the existing systems, implementation of an ontology-based
              knowledge specifically for a domestic service robot has not been
              addressed. Especially when it comes to unknown word
              interpretation, the proposed solution will add more novelty to the
              research outcome, as it can be more challenging when considered
              about the dynamic nature of a domestic environment. As an example,
              if a robot is given an instruction ‘Bring me the cup’, the robot
              should be able to interpret the word ‘cup’. This entire
              functionality cannot be achieved only through a knowledge model
              whereas there should be necessary implementations for object
              identification given a proper definition and related knowledge
              descriptions. Therefore, in this case, the bridging between
              ontology model and the image processing and object detection
              engine/component plays an integral role. The recognition of
              objects under uncontrolled, real-world conditions is of paramount
              importance in robotics [16]. In this work, object recognition
              refers to the recognition of a specific object class/category
              (e.g. a cup). Integration of the vision-based algorithms can
              enhance the performance and the efficiency of the system [17]. The
              work presented in [18] critically explains the basic algorithms to
              be addressed before applying image processing techniques. These
              techniques include; image enhancement, noise reduction etc. The
              goal of this work will be fulfilled with the addition of the
              functionality to facilitate the usability of robotic platforms to
              nonexpert users, hence simplifying the time-consuming process of
              configuring and programming robotic platforms to integrate in
              real-world applications [19]. This is addressed through the
              implementation of the proposed web interface connecting ROS
              environment and the system functionalities.
              <br />
              <br />
              Therefore, In our proposed system, the major attempt is to
              implement a methodology to cope with uncertain terms related to
              the speed of movements and uncertain terms related to time as well
              as unknown words that are included in a natural language command
              by a domestic service robot. Additionally, a user-friendly
              interface for the system was implemented
            </p>
          </div>
        </div>

        <div class="row">
          <div
            class="col-md-12 wow fadeInLeft"
            data-wow-duration=".5s"
            data-wow-delay=".2s"
          >
            <h3>Research Gap</h3>
            <p>
              After referring number of research papers from this area of study,
              we have identified that this research would provide a novel
              approach to the problems encountered by the current users of
              domestic service robots. Mainly, the proposed solution
              incorporated solutions for three main problems identified, namely,
            </p>
            <ul class="list-unstyled">
              <li>
                <i
                  data-vi="check"
                  data-vi-size="40"
                  data-vi-primary="#1992ec"
                  data-vi-accent="#daeffd"
                  data-vi-prop="#CEFAFF"
                ></i>
                <span
                  >Interpretation of uncertain terms related to the speed of
                  movements</span
                >
              </li>
              <li>
                <i
                  data-vi="check"
                  data-vi-size="40"
                  data-vi-primary="#1992ec"
                  data-vi-accent="#daeffd"
                  data-vi-prop="#CEFAFF"
                ></i>
                <span>Interpretation of uncertain terms related to time</span>
              </li>
              <li>
                <i
                  data-vi="check"
                  data-vi-size="40"
                  data-vi-primary="#1992ec"
                  data-vi-accent="#daeffd"
                  data-vi-prop="#CEFAFF"
                ></i>
                <span>Interpretation of unknown word/object</span>
              </li>
            </ul>
            <p>
              This work addresses the task that minimize the challenges of the
              non-technical users when interacting with ROS robots. We are
              hoping to implement a web interface for the non-technical users by
              providing a teleoperation control panel which will allow users to
              interact and control the robot. The robots that will be using in
              this work can be named as Turtlebot. Considering the time
              management and the budget management planned to run the Turtlebot
              in a simulation environment. A simulation environment approach in
              developing robots can be very time saving. We have used the gazebo
              simulator as the simulation environment. ROS provides libraries
              and tools help software developers create robot applications. It
              provides hardware abstraction, device drivers, libraries and many
              more. Along with ROS gazebo is used for simulation. A
              well-designed simulator makes it possible to rapidly test
              algorithms, design robots, and perform regression testing using
              realistic scenarios. Gazebo offers the ability to simulate
              populations of robots accurately and efficiently in complex indoor
              and outdoor environments.
            </p>
          </div>
        </div>

        <div class="row">
          <div
            class="col-md-12 wow fadeInLeft"
            data-wow-duration=".5s"
            data-wow-delay=".2s"
          >
            <h3>Research Problem</h3>
            <p>
              Assistive robots have been developed to improve the living
              standards of people. These assistive robots are intended to be
              operated by non-expert users. Hence, they should have the ability
              to interact with humans in a human-friendly manner. Humans prefer
              to use voice instructions, responses, and suggestions in their
              daily interactions. Such voice instructions and responses often
              include uncertain terms and lexical symbols rather than precise
              quantitative values. <br /><br />
              Adapting the perception of the uncertain terms related to speed of
              movements such as “very slowly”, “slowly”, “very fast”, uncertain
              terms related to time such as “later”, “sooner”, “now” and
              interpretation of unknown words included in instructions that are
              not within the range of known vocabulary are major situation where
              an improvement is needed for a user friendly interaction between a
              robot and a human user.
              <br /><br />
              For example, a human will issue a command “go slowly” rather than
              saying “go at a speed of 1mph”. The quantitative word “slowly” is
              unclear. Other than that, if a robot is commanded to “Bring the
              Key” and if it does not know the exact definition of the word
              ‘Key’ again the same problematic situation could arouse.
            </p>
          </div>
        </div>

        <div class="row">
          <div
            class="col-md-12 wow fadeInLeft"
            data-wow-duration=".5s"
            data-wow-delay=".2s"
          >
            <h3>Research Objective</h3>
            <p>
              The main objective of this research project is to implement a
              methodology to cope with uncertain terms related to speed of
              movements and uncertain terms related to time and unknown words
              included in instructions to increase the human-friendly
              interactive features in a domestic service robot.
            </p>

            <p>
              To accomplish this main objective serval specific objectives
              mentioned below should be achieved.
            </p>

            <ul class="list-unstyled">
              <li>
                <i
                  data-vi="check"
                  data-vi-size="40"
                  data-vi-primary="#1992ec"
                  data-vi-accent="#daeffd"
                  data-vi-prop="#CEFAFF"
                ></i>
                <span
                  >Introduce a methodology to interpret uncertain terms related
                  to the speed of movements contained in natural language
                  instructions.</span
                >
              </li>
              <li>
                <i
                  data-vi="check"
                  data-vi-size="40"
                  data-vi-primary="#1992ec"
                  data-vi-accent="#daeffd"
                  data-vi-prop="#CEFAFF"
                ></i>
                <span
                  >Introduce a methodology to interpret uncertain terms related
                  to time contained in natural language instructions.</span
                >
              </li>
              <li>
                <i
                  data-vi="check"
                  data-vi-size="40"
                  data-vi-primary="#1992ec"
                  data-vi-accent="#daeffd"
                  data-vi-prop="#CEFAFF"
                ></i>
                <span
                  >Investigate improvements that can be acquired by using fuzzy
                  type II systems for interpreting uncertain information and
                  evaluate both fuzzy type I and the fuzzy type II systems and
                  use the approach which fits the best.</span
                >
              </li>
              <li>
                <i
                  data-vi="check"
                  data-vi-size="40"
                  data-vi-primary="#1992ec"
                  data-vi-accent="#daeffd"
                  data-vi-prop="#CEFAFF"
                ></i>
                <span
                  >Analyze and interpret an unknown word and refer to the exact
                  object/task precisely using the definitions of the ontology
                  model and visual inputs of the scanned environment using image
                  processing techniques</span
                >
              </li>
              <li>
                <i
                  data-vi="check"
                  data-vi-size="40"
                  data-vi-primary="#1992ec"
                  data-vi-accent="#daeffd"
                  data-vi-prop="#CEFAFF"
                ></i>
                <span
                  >To implement a user-friendly interface for non-technical
                  users to let the users to get familiar with the robotic
                  environment and minimize the confusion of the using of the
                  robots.</span
                >
              </li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <section id="brands" style="padding-top: 70px">
      <div class="container">
        <div class="row">
          <div class="col-xs-12 text-center title-container">
            <!-- /// Title /// -->
            <h2 class="section-title">Metholodology</h2>
          </div>
        </div>
        <br />
        <div class="section light-bg" id="documentation">
          <div class="container">
            <div class="card-deck">
              <div class="card">
                <img src="imgs/5.png" class="card-img-top" alt="..." />
                <div class="slide-card">
                  <div class="row">
                    <div class="col-md-12">
                      <h4 style="color: #212529">System Oveview</h4>
                      <p class="card-text" style="margin: 12px 0 0 0">
                        <small class="text-muted"
                          >In brief, project Bot-Intel is mainly focusing on
                          interpreting uncertain terms related to the speed of
                          the moment and the uncertain terms related to time and
                          identifying unknown objects within the domestic
                          environment and implementing a user interface for
                          users to learn ROS faster.
                        </small>
                      </p>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            <br />
            <div class="card-deck">
              <div class="card">
                <img src="imgs/6.png" class="card-img-top" alt="..." />
                <div class="slide-card" style="height: 100%">
                  <div class="row">
                    <div class="col-md-12">
                      <h4 style="color: #212529">
                        Interpreting Uncertain Information related to Speed of
                        Movement contained in Natural Language Instructions
                      </h4>
                      <p class="card-text" style="margin: 12px 0 0 0">
                        <small class="text-muted"
                          >This image represents the process of interpreting
                          uncertain terms related to speed of the movements step
                          by step.</small
                        >
                      </p>
                    </div>
                  </div>
                </div>
              </div>
              <div class="card">
                <img
                  src="imgs/7.png"
                  class="card-img-top"
                  alt="..."
                  style="padding-bottom: 14px"
                />
                <div class="slide-card" style="height: 100%">
                  <div class="row">
                    <div class="col-md-12">
                      <h4 style="color: #212529">
                        Interpreting Uncertain Information related to Time
                        contained in Natural Language Instructions
                      </h4>
                      <p class="card-text" style="margin: 12px 0 0 0">
                        <small class="text-muted"
                          >This image represents the process of interpreting
                          uncertain terms related to time step by step.
                          <br />
                        </small>
                      </p>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            <br />
            <div class="card-deck">
              <div class="card">
                <img src="imgs/2.png" class="card-img-top" alt="..." />
                <div class="slide-card" style="height: 100%">
                  <div class="row">
                    <div class="col-md-12">
                      <h4 style="color: #212529">
                        Interpret Unknown words contained in Natural Language
                        Instructions using robotic knowledge and object
                        detection using a Ontology Model
                      </h4>
                      <p class="card-text" style="margin: 12px 0 0 0">
                        <small class="text-muted"
                          >This component has two segments. The first segment
                          contains knowledge description represented as an
                          ontology model that describes knowledge classes with
                          reference to the domestic context and the second
                          segment models the necessary relationships in between
                          the ontology model and image processing component in
                          ROS environment</small
                        >
                      </p>
                    </div>
                  </div>
                </div>
              </div>
              <div class="card">
                <img src="imgs/3.png" class="card-img-top" alt="..." />
                <div class="slide-card" style="height: 100%">
                  <div class="row">
                    <div class="col-md-12">
                      <h4 style="color: #212529">
                        Object Detection using Image Processing Component
                      </h4>
                      <p class="card-text" style="margin: 12px 0 0 0">
                        <small class="text-muted"
                          >This component consists of two parts. They are
                          namely, Object detection model and underlying Neural
                          Network (NN) architecture. The Object detection
                          component was developed using the OpenCV, Python,
                          Pytorch and ONNX libraries to be used with the camera
                          component of tutleBot3 in Gazebo simulator.
                        </small>
                      </p>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            <br />
            <div id="custom-card-deck" class="card-deck">
              <div class="card">
                <img src="imgs/4.png" class="card-img-top" alt="..." />
                <div class="slide-card" style="height: 100%">
                  <div class="row">
                    <div class="col-md-12">
                      <h4 style="color: #212529">
                        Implementing a user friendly interface for ROS
                        Application
                      </h4>
                      <p class="card-text" style="margin: 12px 0 0 0">
                        <small class="text-muted"
                          >BOT-INTEL web interface is implement -ed on the ROS-
                          turtlebot platform. Installed the rosbridge_suite to
                          handle JSON commands in the process of converting the
                          relevant commands to ROS commands. Used
                          video_web_server for video streamin -g to display the
                          live camera stream of the robot’s camera in the
                          interface.</small
                        >
                      </p>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section id="brands">
      <div class="container">
        <div class="row">
          <div
            class="col-xs-12 col-sm-8 col-md-6 col-sm-offset-2 col-md-offset-4 text-center title-container"
          >
            <!-- /// Title /// -->
            <h2 class="section-title">Technologies Used</h2>
          </div>
        </div>
        <div class="row">
          <div class="col-xs-12 text-center title-container">
            <img src="imgs/brands/1.png" class="img-fluid" alt="brand-1" />
            <img src="imgs/brands/2.png" class="img-fluid" alt="brand-1" />
            <img src="imgs/brands/3.png" class="img-fluid" alt="brand-3" />
            <img src="imgs/brands/4.png" class="img-fluid" alt="brand-3" />
            <img src="imgs/brands/5.png" class="img-fluid" alt="brand-3" />
          </div>
          <div class="col-xs-12 text-center title-container">
            <img src="imgs/brands/6.png" class="img-fluid" alt="brand-1" />
            <img src="imgs/brands/7.png" class="img-fluid" alt="brand-1" />
            <img src="imgs/brands/8.png" class="img-fluid" alt="brand-3" />
            <img src="imgs/brands/10.png" class="img-fluid" alt="brand-3" />
          </div>
        </div>
      </div>
    </section>
    <section id="services" style="padding-bottom: 50px">
      <div class="container">
        <div class="row">
          <div
            class="col-md-12 wow fadeInLeft"
            data-wow-duration=".5s"
            data-wow-delay=".2s"
          >
            <h3>References</h3>
            <p>
              [1] "Global Domestic Service Robots Market Size and Trends to
              2020",Kennethresearch.com,2020.[Online].Available:https://www.kennethresearch.com/report-details/domestic-service-robots
              market/10085638. [Accessed: 19- Feb- 2020]
              <br />
              [2] T. Fong, I. Nourbakhsh and K. Dautenhahn, "A survey of
              socially interactive robots", Robotics and Autonomous Systems,
              vol. 42, no. 3-4, pp. 143-166, 2003. Available:
              10.1016/s0921-8890(02)00372-x [Accessed 20 February 2020].
              <br />
              [3] P. Menzel and F. d'Aluisio, Robo Sapiens: Evolution of a new
              Species. Cambridge, MA, USA: MIT Press, 2001.
              <br />
              [4] S. Frennert and B. Östlund, "Review: Seven Matters of Concern
              of Social Robots and Older People", International Journal of
              Social Robotics, vol. 6, no. 2, pp. 299-310, 2014. Available:
              10.1007/s12369-013-0225-8 [Accessed 20 February 2020].
              <br />
              [5] S. Kleanthous et al., “Analysis of elderly users preferences
              and expectations on service robots personality, appearance and
              interaction,” in Proc. Int. Conf. Human Aspects IT Aged
              Population, 2016, pp. 30-45.
              <br />
              [6] S. Schiffer, A. Ferrein and G. Lakemeyer, "Reasoning with
              Qualitative Positional Information for Domestic Domains in the
              Situation Calculus", Journal of Intelligent & Robotic Systems,
              vol. 66, no. 1-2, pp. 273-300, 2011. Available:
              10.1007/s10846-011-9606-0 [Accessed 22 February 2020].
              <br />
              [7] C. Jayawardena, K. Watanabe and K. Izumi, "Controlling a robot
              manipulator with fuzzy voice commands using a probabilistic neural
              network", Neural Computing and Applications, vol. 16, no. 2, pp.
              155-166, 2006. Available: 10.1007/s00521-006-0056-8 [Accessed 20
              February 2020].
              <br />
              [8] A. Jayasekara, K. Watanabe, K. Kiguchi and K. Izumi,
              "Adaptation of robot’s perception of fuzzy linguistic information
              by evaluating vocal cues for controlling a robot manipulator",
              Artificial Life and Robotics, vol. 15, no. 1, pp. 5-9, 2010.
              Available: 10.1007/s10015-010-0755-1 [Accessed 22 February 2020].
              <br />
              [9] M. A. V. J. Muthugala and A. G. B. P. Jayasekara, “Enhancing
              human robot interaction by interpreting uncertain information in
              navigational commands based on experience and environment,” in
              Proc. IEEE Int. Conf.Robot. Autom. (ICRA), May 2016, pp.
              2915_2921.
              <br />
              [10] M. Skubic et al., "Spatial Language for Human–Robot Dialogs",
              IEEE Transactions on Systems, Man and Cybernetics, Part C
              (Applications and Reviews), vol. 34, no. 2, pp. 154-167, 2004.
              Available: 10.1109/tsmcc.2004.826273 [Accessed 22 February 2020].
              <br />
              [11] C. Jayawardena, K. Watanabe, and K. Izumi, “Learning of
              object identification by robots commanded by natural language,''
              in Proc. IAS, 2006, pp. 913_920.
              <br />
              [12] M. A. V. J. Muthugala and A. G. B. P. Jayasekara,
              “Synthesizing fuzzy linguistic vocal responses by adapting
              perception of robot based on visual attention,” in Proc. 7th Int.
              Conf. Inf. Autom. Sustainability, Dec. 2014, pp. 16.
              <br />
              [13] A. Olivares-Alarcos et al., "A review and comparison of
              ontology-based approaches to robot autonomy", The Knowledge
              Engineering Review, vol. 34, 2019. Available:
              10.1017/s0269888919000237.
              <br />
              [14] R. Mendoza and M. Williams, “Ontology based object
              categorisation for robots,” in Proc. Conf. Res. Practice Inf.
              Technol. Ser., 2005, vol. 58, pp. 65–80.
              <br />
              [15] Y. C. Go and J. Sohn, “Context modeling for intelligent robot
              services using rule and ontology,” in Proc. 7th ICACT, 2005, vol.
              2, pp. 813–816.
              <br />
              [16] Loncomilla, P., Ruiz-del-Solar, J. and Martínez, L., 2016.
              Object recognition using local invariant features for robotic
              applications: A survey. Pattern Recognition, 60, pp.499-514.
              <br />
              [17] R. Kumar, S. Lal, S. Kumar and P. Chand, "Object Detection
              and Recognition for a Pick and Place Robot", IEEE Asia-Pacific
              World Congress on Computer Science and Engineering 2014, At
              Plantation Island, Fiji, 2015. [Accessed 27 July 2020].
              <br />
              [18] T. P. Cabre, M. T. Cairol, D. F. Calafell, M. T. Ribes, and
              J. P. Roca,"Project-Based Learning Example: Controlling an
              Educational Robotic Arm with Computer Vision," Tecnologias del
              Aprendizaje, IEEE Revista Iberoamericana de, vol. 8, pp. 135-142,
              2013.
              <br />
              [19] Corcho, O., Garc´ıa-Castro, R.: Five challenges for the
              semantic sensor web. SemanticWeb (2010).
            </p>
          </div>
        </div>
      </div>
    </section>

    <section id="footer">
      <div class="container">
        <p>&copy; All Right Reserved 2020. Bot Intel</p>
      </div>
    </section>

    <!-- jQuery -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <!-- Popper.js -->
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
      integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
      crossorigin="anonymous"
    ></script>
    <!-- Bootstrap JS -->
    <script src="bootstrap-4.1.1-dist/js/bootstrap.min.js"></script>
    <!-- Icons -->
    <script src="https://cdn.jsdelivr.net/npm/vivid-icons"></script>
    <script src="https://unpkg.com/vivid-icons"></script>
    <!-- OWL Carousel -->
    <script src="js/plugins/owl-carousel/owl.carousel.js"></script>
    <!-- Video Pop Up Plugin -->
    <script src="js/plugins/YouTube_PopUp-master/YouTubePopUp.jquery.js"></script>
    <script src="js/plugins/wow/wow.min.js"></script>
    <!-- Easing -->
    <script src="js/plugins/jquery.easing.min.js"></script>
    <!-- Main JS -->
    <script src="js/main.js"></script>
  </body>
</html>
